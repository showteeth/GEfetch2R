#' Show Metadata of scRNA-seq Datasets in PanglaoDB.
#'
#' @param specie The specie of the datasets, choose from "Homo sapiens", "Mus musculus", one or multiple value. Default: NULL (All).
#' @param protocol Protocol used to generate the datasets, choose from "10x chromium", "drop-seq", "microwell-seq",
#' "C1 Fluidigm", "inDrops", "Smart-seq2", "CEL-seq", one or multiple value. Default: NULL (All).
#' @param tissue The tissue of the datasets. Default: NULL (All).
#' @param show.cell.type Logical value, whether to show inferred cell type. Default: TRUE.
#'
#' @return Dataframe contains SRA, SRS, Tissue, Protocol, Specie, Cells, CellType (inferred).
#' @importFrom magrittr %>%
#' @importFrom rPanglaoDB getSampleList getSampleComposition
#' @importFrom dplyr filter
#' @export
#'
#' @examples
#' # ShowPanglaoDBMeta(specie = "Homo sapiens", protocol = c("Smart-seq2", "10x chromium"))
ShowPanglaoDBMeta <- function(specie = NULL, protocol = NULL, tissue = NULL, show.cell.type = TRUE) {
  # get all sample metadata
  all.meta <- rPanglaoDB::getSampleList()
  # modify SMART-seq2 to Smart-seq2
  all.meta$Protocol <- gsub(pattern = "SMART-seq2", replacement = "Smart-seq2", x = all.meta$Protocol)

  # no attribute filter
  if (is.null(specie) && is.null(protocol) && is.null(tissue)) {
    used.meta <- all.meta
  }
  # test specie
  if (!is.null(specie)) {
    valid.specie <- intersect(specie, unique(all.meta$Species))
    if (length(valid.specie) == 0) {
      warning("Please provide valid specie! The returned dataframe contains metadata without specie filtering, please check!")
      used.meta <- all.meta
    } else {
      used.meta <- all.meta %>% dplyr::filter(Species %in% valid.specie)
    }
  } else {
    used.meta <- all.meta
  }
  # test protocol
  if (!is.null(protocol)) {
    valid.protocol <- intersect(protocol, unique(used.meta$Protocol))
    if (length(valid.protocol) == 0) {
      warning("Please provide valid protocol! The returned dataframe contains metadata without protocol filtering, please check!")
    } else {
      used.meta <- used.meta %>% dplyr::filter(Protocol %in% valid.protocol)
    }
  }
  # test tissue
  if (!is.null(tissue)) {
    valid.tissue <- intersect(tissue, unique(used.meta$Tissue))
    if (length(valid.tissue) == 0) {
      warning("Please provide valid tissue! The returned dataframe contains metadata without tissue filtering, please check!")
    } else {
      used.meta <- used.meta %>% dplyr::filter(Tissue %in% valid.tissue)
    }
  }
  # get sample cell type
  if (show.cell.type) {
    if (nrow(used.meta) > 0) {
      used.meta.ct <- sapply(used.meta$SRS, function(x) {
        tryCatch(
          {
            x.com <- rPanglaoDB::getSampleComposition(srs = x, verbose = FALSE)
            paste(unique(x.com[["Cell Type"]]), collapse = ", ")
          },
          error = function(e) {
            message("This is an error when obtaining inferred cell types of ", x)
            "None"
          }
        )
      })
      used.meta.ct.df <- as.data.frame(used.meta.ct)
      colnames(used.meta.ct.df) <- "CellType"
      used.meta <- merge(used.meta, used.meta.ct.df, by.x = "SRS", by.y = 0, all.x = TRUE)
      used.meta <- used.meta[c("SRA", "SRS", "Tissue", "Protocol", "Species", "Cells", "CellType")]
    }
  }
  # replace srs with '=' with notused
  used.meta$SRS = gsub(pattern = "nSRS=[0-9]*", replacement = "notused", x = used.meta$SRS)
  return(used.meta)
}



#' Parse PanglaoDB Data.
#'
#' @param meta Metadata contains "SRA", "SRS", "Tissue", "Protocol", "Species", can be obtained with \code{ShowPanglaoDBMeta}.
#' @param cell.type Extract samples with specified cell types. For samples without SRS (notused), this value can only be "All" or "None", or
#' these samples will be filtered. Default: "All".
#' @param include.gene Include cells expressing the genes. Default: NA.
#' @param exclude.gene Exclude cells expressing the genes. Default: NA.
#' @param merge Logical value, whether to merge Seurat list. Default: FALSE.
#'
#' @return Seurat object (if \code{merge} is TRUE) or list of Seurat objects (if \code{merge} is FALSE).
#' @importFrom magrittr %>%
#' @importFrom rPanglaoDB getSampleList getSampleComposition getSamples
#' @importFrom dplyr filter
#' @importFrom pbapply pbapply
#' @importFrom Matrix rowSums colMeans
#' @importFrom Seurat CreateSeuratObject
#' @importFrom utils read.table
#' @export
#'
#' @examples
#' # hsa.meta = ShowPanglaoDBMeta(specie = "Homo sapiens", protocol = c("Smart-seq2", "10x chromium"), show.cell.type = TRUE)
#' # hsa.seu = ParsePanglaoDB(hsa.meta, merge = TRUE)
ParsePanglaoDB = function(meta, cell.type = "All", include.gene = NA, exclude.gene = NA, merge = FALSE){
  # check columns
  CheckColumns(df = meta, columns = c("SRA", "SRS", "Tissue", "Protocol", "Species"))

  # split meta to save time
  meta.normal = meta %>% dplyr::filter(SRS != "notused")
  meta.abnormal = meta %>% dplyr::filter(SRS == "notused")

  # get Seurat object
  ## normal
  if(nrow(meta.normal)>0){
    message("Processing ", nrow(meta.normal), " samples with SRS!")
    normal.seu = rPanglaoDB::getSamples(sra = meta.normal$SRA, srs = meta.normal$SRS, tissue = meta.normal$Tissue,
                                        protocol = meta.normal$Protocol, specie = meta.normal$Species, celltype = cell.type,
                                        include = include.gene, exclude = exclude.gene, merge = FALSE)
  }else{
    normal.seu = list()
  }
  ## abnormal
  if(nrow(meta.abnormal)>0){
    if(cell.type != "All" && cell.type != "None"){
      message("There is no ", cell.type, " in samples without SRS (notused)!")
      abnormal.seu = list()
    }else{
      message("Processing ", nrow(meta.abnormal), " samples without SRS (notused)!")
      abnormal.seu = getSamples_internal(sra = meta.abnormal$SRA, srs = meta.abnormal$SRS, tissue = meta.abnormal$Tissue,
                                         protocol = meta.abnormal$Protocol, specie = meta.abnormal$Species, celltype = cell.type,
                                         include = include.gene, exclude = exclude.gene, merge = FALSE)
    }
  }else{
    abnormal.seu = list()
  }

  # merge list
  meta.seu = c(normal.seu, abnormal.seu)
  # merge or not
  if(merge){
    meta.seu <- mergeExperiments(meta.seu)
  }
  return(meta.seu)
}

# get samples without SRS
getSamples_internal <- function(sra = 'All', srs = 'All', tissue = 'All', protocol = 'All', specie = 'All', celltype='All', include = NA, exclude = NA, merge = TRUE){
  # SampleList
  sampleList <- rPanglaoDB::getSampleList()
  sampleList$SRS = gsub(pattern = "nSRS=[0-9]*", replacement = "notused", x = sampleList$SRS)

  # Filters
  SRA <- match.arg(arg = sra, choices = unique(c('All',sampleList$SRA)), several.ok = TRUE)
  if(isTRUE('All' %in% SRA)){
    SRA <- unique(sampleList$SRA)
  }
  SRS <- match.arg(arg = srs, choices = unique(c('All', 'notused', sampleList$SRS)), several.ok = TRUE)
  if(isTRUE('All' %in% SRS)){
    SRS <- unique(sampleList$SRS)
  }

  Tissue <- match.arg(arg = tissue, choices = unique(c('All',sampleList$Tissue)), several.ok = TRUE)
  if(isTRUE('All' %in% Tissue)){
    Tissue <- unique(sampleList$Tissue)
  }
  Protocol <- match.arg(arg = protocol, choices = unique(c('All',sampleList$Protocol)), several.ok = TRUE)
  if(isTRUE('All' %in% Protocol)){
    Protocol <- unique(sampleList$Protocol)
  }
  Specie <- match.arg(arg = specie, choices = unique(c('All',sampleList$Species)), several.ok = TRUE)
  if(isTRUE('All' %in% Specie)){
    Specie <- unique(sampleList$Species)
  }

  # Applying filter
  F1 <- sampleList$SRA %in% SRA
  F2 <- sampleList$SRS %in% SRS
  F3 <- sampleList$Tissue %in% Tissue
  F4 <- sampleList$Protocol %in% Protocol
  F5 <- sampleList$Species %in% Specie
  sampleList <- sampleList[F1 & F2 & F3 & F4 & F5,]

  # Error
  if (nrow(sampleList) == 0){
    message('0 Samples Found')
    return()
  }

  # Filtering by cell-type
  ctList.raw = lapply(1:nrow(sampleList), function(x){
    if(sampleList[x,"SRS"] == "notused"){
      sc = sampleList[x, ]
      sc[["Cluster"]] = "None"
      sc[["Cell Type"]] = "None"
      sc[c("SRA", "SRS", "Tissue", "Protocol", "Species", "Cluster", "Cells", "Cell Type")]
    }else{
      rPanglaoDB::getSampleComposition(srs = sampleList[x,"SRS"], verbose = FALSE)
    }
  })
  ctList = do.call(rbind, ctList.raw)
  rownames(ctList) = NULL

  CellType <- match.arg(arg = celltype, choices = unique(c('All', ctList$`Cell Type`)), several.ok = TRUE)
  if(isTRUE('All' %in% CellType)){
    CellType <- unique(ctList$`Cell Type`)
  }
  ctList <- ctList[ctList$`Cell Type` %in% CellType,]
  sampleList <- sampleList[sampleList$SRS %in% ctList$SRS,]

  # Error
  if (nrow(sampleList) == 0){
    message('0 Samples Found')
    return()
  }

  dataSets <- pbapply::pbapply(sampleList,1, function(X){
    oConnection <- paste0("https://panglaodb.se/data_dl.php?sra=",X[1],"&srs=",X[2],"&filetype=R&datatype=readcounts")
    oConnection <-  url(oConnection, headers = list(
      `Connection` = 'keep-alive',
      `User-Agent` =  "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0"
    ))
    try(load(oConnection), silent = TRUE)
    if(exists('sm')){
      rownames(sm) <- unlist(lapply(strsplit(rownames(sm), '-ENS|_ENS'), function(X){X[1]}))
      sm <- sm[Matrix::rowSums(sm) > 0,]
      if(X[2] == "notused"){
        sm <- suppressWarnings(Seurat::CreateSeuratObject(sm, project = paste(as.character(X[1]), as.character(X[2]), sep = "_")))
        cellTypes = "None"
        cClusters = NA
      }else{
        cNames <- rPanglaoDB::getSampleComposition(srs = as.character(X[2]), verbose = FALSE)
        rownames(cNames) <- cNames$Cluster
        tempFile <- tempfile()
        download.file(paste0('https://panglaodb.se/data_dl.php?sra=',X[1],'&srs=',X[2],'&datatype=clusters&filetype=txt'),
                      destfile = tempFile, quiet = TRUE,
                      headers = list(
                        `Connection` = 'keep-alive',
                        `User-Agent` =  "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0"))
        cClusters <- utils::read.table(tempFile, row.names = 1)
        sm <- sm[,colnames(sm) %in% rownames(cClusters)]
        cClusters <- cClusters[colnames(sm),]
        names(cClusters) <- colnames(sm)
        # Capital gene names to allow integration across Human and Mice
        rownames(sm) <- toupper(rownames(sm))
        sm <- suppressWarnings(Seurat::CreateSeuratObject(sm, project = as.character(X[2])))
        cellTypes <- cNames[as.character(cClusters),]$`Cell Type`
        names(cellTypes) <- colnames(sm)
      }
      sm$Sample <- sm$orig.ident
      sm$CellTypes <- cellTypes
      sm$panglaoCluster <- as.character(cClusters)
      sm$Tissue <- X[['Tissue']]
      sm <- subset(sm, cells = colnames(sm)[sm$CellTypes %in% CellType])
      sm$CellTypes[sm$CellTypes %in% 'Unknown'] <- NA
      sm$CellTypes[sm$CellTypes %in% 'None'] <- NA
      sm$Specie <- X[['Species']]

      # Filtering by genes
      include <- include[include %in% rownames(sm@assays$RNA@counts)]
      exclude <- exclude[exclude %in% rownames(sm@assays$RNA@counts)]
      filterCells <- FALSE
      if(length(include) > 0){
        include <- Matrix::colMeans(sm@assays$RNA@counts[include, , drop = FALSE] != 0) == 1
        filterCells <- TRUE
      } else{
        include <- rep(TRUE, ncol(sm))
      }
      if(length(exclude) > 0){
        exclude <- Matrix::colMeans(sm@assays$RNA@counts[exclude, , drop = FALSE] != 0) != 0
        filterCells <- TRUE
      } else {
        exclude <- rep(FALSE, ncol(sm))
      }
      if(isTRUE(filterCells)){
        filterCells <- (include & !exclude)
        if(any(filterCells)){
          sm <- subset(sm, cells = colnames(sm)[filterCells])
        } else {
          sm <- list()
        }
      }
      close.connection(oConnection)
    } else {
      sm <- list()
    }
    return(sm)
  })
  names(dataSets) <- ifelse(sampleList$SRS == "notused", paste(sampleList$SRA, sampleList$SRS, sep = "_"), sampleList$SRS)

  dataSets <- dataSets[unlist(lapply(dataSets, class)) %in% 'Seurat']

  if(isTRUE(merge)){
    dataSets <- mergeExperiments(dataSets)
  }
  return(dataSets)
}
# merge Seurat object
mergeExperiments <- function(experimentList){
  for(i in seq_along(experimentList)[-1]){
    experimentList[[1]] <- suppressWarnings(merge(experimentList[[1]], experimentList[[i]]))
    experimentList[[i]] <- new('Seurat')
  }
  experimentList <- experimentList[[1]]
  return(experimentList)
}

CheckColumns = function(df, columns){
  if (!all(columns %in% colnames(df))) {
    miss.cols = setdiff(columns, colnames(df))
    stop(paste0(paste(miss.cols, collapse = ", "), " does not exist, Please Check!"))
  }
}

# 测试 ------
library(tidyverse)
library(rPanglaoDB)
hsa.meta = ShowPanglaoDBMeta(specie = "Homo sapiens", protocol = c("Smart-seq2", "10x chromium"), show.cell.type = TRUE)
hsa.seu = ParsePanglaoDB(hsa.meta[1:10,], merge = TRUE)

# 处理zenodo --------
# 使用zen4R
# remotes::install_github("eblondel/zen4R")
library(zen4R)
zenodo <- ZenodoManager$new(logger = "INFO")
rec <- zenodo$getRecordByDOI("10.5281/zenodo.3378733")
files <- rec$listFiles(pretty = TRUE)
#create a folder where to download my files
setwd("/Users/soyabean/Desktop/tmp/scdown/")
dir.create("download_zenodo")
#download files
rec$downloadFiles(path = "download_zenodo")
downloaded_files <- list.files("download_zenodo")
download_zenodo(path = "download_zenodo", "10.5281/zenodo.3378733")

# 查询
query_sc = zenodo$getRecords(q = "single cell")

# * 自己写函数 -----
# 注意的点：
# * 使用的是api，而不是单纯的网址 (https://developers.zenodo.org/?python#requests)
# * 文件类型都是小写
# * 使用fromJSON可以一块得到所有的文件信息

response <- curl::curl_fetch_memory("https://zenodo.org/api/records/7243603")
content = jsonlite::fromJSON(rawToChar(response$content))

# RData
response <- curl::curl_fetch_memory("https://zenodo.org/api/records/7244441")
content = jsonlite::fromJSON(rawToChar(response$content))

# Restricted Access
response <- curl::curl_fetch_memory("https://zenodo.org/api/records/48065")
content = jsonlite::fromJSON(rawToChar(response$content))
# $metadata$access_right: open
# $metadata$access_right: restricted

# rds文件
response <- curl::curl_fetch_memory("https://zenodo.org/api/records/3233870")
content = jsonlite::fromJSON(rawToChar(response$content))

# 有两个点：7586958
response <- curl::curl_fetch_memory("https://zenodo.org/api/records/7586958")
content = jsonlite::fromJSON(rawToChar(response$content))

library(tidyverse)
doi = "10.5281/zenodo.7243603"
file.ext = "rdata"

#' Prepare Dataframe with Zenodo DOI.
#'
#' @param doi Zenodo DOI, should start with "10.5281/zenodo.".
#' @param file.ext The valid file extension for download. When NULL, use all files. Default: c("rdata", "h5ad").
#'
#' @return Dataframe contains files with valid extension in given Zenodo DOI.
#' @importFrom magrittr %>%
#' @importFrom curl curl_fetch_memory
#' @importFrom jsonlite fromJSON
#' @importFrom dplyr filter
#' @export
#'
#' @examples
#' # zebraffish.df = PrepareZenodo(doi = "10.5281/zenodo.7243603")
#' # PrepareZenodo(doi = "10.5281/zenodo.48065") # Restricted Access
PrepareZenodo = function(doi, file.ext = c("rdata", "h5ad")){
  # prepare link
  record.id = gsub(pattern = "10.5281/zenodo.", replacement = "", x = doi, fixed = TRUE)
  record.api = paste0("https://zenodo.org/api/records/", record.id)
  # download page info
  record.page <- curl::curl_fetch_memory(record.api)
  # convert to string
  record.content = jsonlite::fromJSON(rawToChar(record.page$content))
  # access status
  record.access = record.content$metadata$access_right
  if(record.access == "open"){
    # extract files
    record.files = record.content$files
    # filter files
    if(is.null(file.ext)){
      record.files.used = record.files
    }else{
      record.files.used = record.files %>% dplyr::filter(type %in% file.ext)
    }
    # check the data
    if(nrow(record.files.used) == 0){
      return(NULL)
    }
    # prepare md5sum
    record.files.used$checksum = gsub(pattern = "md5:", replacement = "", record.files.used$checksum)
    record.files.used.final = data.frame(title = record.content$metadata$title, description = record.content$metadata$description,
                                         url = record.files.used$links$self, filename = basename(record.files.used$links$self),
                                         md5 = record.files.used$checksum, license = record.content$metadata$license$id)

  }else{
    message(doi, " is not open access!")
    record.files.used.final = data.frame()
  }
  return(record.files.used.final)
}

DownloadZenodo(doi = c("1111", "10.5281/zenodo.7243603", "10.5281/zenodo.7244441"), file.ext = c("rdata", "rds"), out.folder = "/Users/soyabean/Desktop/tmp/scdown/download_zenodo")
#' Download Data with Zenodo DOI.
#'
#' @param doi A vector of Zenodo DOIs to download. Default: NULL.
#' @param file.ext The valid file extension for download. When NULL, use all files. Default: c("rdata", "rds", "h5ad").
#' @param doi.df DOI dataframe for download. This is useful when something wrong happens in downloading
#' (e.g. MD5 verification failure, \code{DownloadZenodo} will return a dataframe contains failure terms.). Default: NULL.
#' It is required to provide either \code{doi} or \code{doi.df}.
#' @param out.folder The output folder. Default: NULL (current working directory).
#' @param timeout Maximum request time. Default: 1000.
#' @param quiet Logical value, whether to show downloading progress. Default: FALSE (show).
#' @param parallel Logical value, whether to download parallelly. Default: TRUE. When "libcurl" is available for \code{download.file},
#' the parallel is done by default (\code{parallel} can be FALSE).
#'
#' @return When successful, NULL. When MD5 verification failure, a dataframe contains failure terms.
#' @importFrom magrittr %>%
#' @importFrom curl curl_fetch_memory
#' @importFrom jsonlite fromJSON
#' @importFrom dplyr filter
#' @importFrom parallel detectCores mclapply
#' @importFrom utils download.file
#' @importFrom tools md5sum
#' @export
#'
#' @examples
#' # DownloadZenodo(doi = c("1111", "10.5281/zenodo.7243603", "10.5281/zenodo.7244441"), file.ext = c("rdata", "rds"), out.folder = "/path/to/outfoder")
DownloadZenodo = function(doi = NULL, file.ext = c("rdata", "rds", "h5ad"), doi.df = NULL, out.folder = NULL, timeout = 1000,
                          quiet = FALSE, parallel = TRUE){
  if(!is.null(doi.df)){
    doi.df = doi.df
  }else if(!is.null(doi)){
    # check doi
    doi.status = startsWith(x = doi, prefix = "10.5281/zenodo.")
    if(!all(doi.status)){
      wrong.doi = doi[!doi.status]
      message(paste0(wrong.doi, collapse = ", "), " are not valid dois, please check!")
      doi = doi[doi.status]
    }
    # file extension to lower
    file.ext = tolower(file.ext)
    # prepare data frame
    doi.list = lapply(doi, function(x){
      PrepareZenodo(doi = x, file.ext = file.ext)
    })
    doi.df = do.call(rbind, doi.list)
  }else{
    stop("Please provide either doi or doi.df!")
  }
  # prepare output folder
  if(is.null(out.folder)){
    out.folder = getwd()
  }
  doi.df$filename = file.path(out.folder, doi.df$filename)
  # set timeout
  env.timeout = getOption("timeout")
  options(timeout = timeout)
  message("Start downloading!")
  if(isTRUE(parallel)){
    # prepare cores
    cores.used = min(parallel::detectCores(), nrow(doi.df))
    down.status = parallel::mclapply(X = 1:nrow(doi.df), FUN = function(x){
      utils::download.file(url = doi.df[x, "url"], destfile = doi.df[x, "filename"], quiet = quiet, mode = "wb")
    }, mc.cores = cores.used)
  }else{
    down.status = utils::download.file(url = doi.df$url, destfile = doi.df$filename, quiet = quiet, mode = "wb")
  }
  message("Finish downloading!")
  # restore timeout
  options(timeout = env.timeout)
  # check the md5sum
  down.md5 = tools::md5sum(doi.df$filename)
  raw.md5 = doi.df$md5
  names(raw.md5) = doi.df$filename
  md5.check = down.md5 == raw.md5
  if(all(md5.check)){
    message("Download and MD5 verification successful!")
    return(NULL)
  }else{
    wrong.md5.df = doi.df[!md5.check, ]
    # restore filename
    wrong.md5.df$filename = basename(wrong.md5.df$filename)
    message("MD5 verification failure for: ", paste0(wrong.md5.df$filename, collapse = ", "), " . You can re-run DownloadZenodo with doi.df!")
    return(wrong.md5.df)
  }
}


pbapply::pbapply(doi.df, 1, function(x){
  download.file(url = x["url"], destfile = x["filename"], quiet = quiet, mode = "wb")
})
# 使用BiocParallel
# BiocParallel不同模式对比：https://rpubs.com/Phyleria/Introduction_To_BiocParallel
n.cores = 2
library(BiocParallel)
system.time(BiocParallel::bplapply(1:nrow(doi.df), BPPARAM = BiocParallel::MulticoreParam(workers = n.cores, progress = TRUE), FUN = function(x) {
  download.file(url = doi.df[x, "url"], destfile = doi.df[x, "filename"], quiet = quiet, mode = "wb")
}))
# user  system elapsed
# 4.194   7.259 282.335

# 直接使用download.file --- 感觉默认的就是并行下载的(使用libcurl method下载是支持同时下载多个的)
system.time(download.file(url = doi.df$url, destfile = doi.df$filename, quiet = quiet, mode = "wb"))
# user  system elapsed
# 9.160  15.849 253.267

# 使用pbapply，这个是线性的，好处是添加进度条
system.time(pbapply::pbapply(doi.df, 1, function(x){
  download.file(url = x["url"], destfile = x["filename"], quiet = quiet, mode = "wb")
}))
# user  system elapsed
# 8.232  15.363 248.687

# 使用mclapply
system.time(parallel::mclapply(X = 1:nrow(doi.df), FUN = function(x){
  download.file(url = doi.df[x, "url"], destfile = doi.df[x, "filename"], quiet = quiet, mode = "wb")
}, mc.cores = 2))
# user  system elapsed
# 5.076   8.111 247.299

# 针对 UCSC Cell Browser -------------
# https://cells.ucsc.edu/cortex-dev/exprMatrix.tsv.gz
# https://cells.ucsc.edu/cortex-dev/meta.tsv
# https://cells.ucsc.edu/cortex-dev/desc.json
# https://cells.ucsc.edu/evocell/spongilla/desc.json

PasteAttr = function(df, attr){
  for (at in attr){
    df[[at]] = sapply(df[[at]], function(x){ paste0(x, collapse = ", ") })
  }
  return(df)
}

all.datasets.json = jsonlite::fromJSON(txt = "https://cells.ucsc.edu/dataset.json")
all.datasets.df = jsonlite::flatten(all.datasets.json$datasets)
colnames(all.datasets.df) = gsub(pattern = ".*\\.", replacement = "", x = colnames(all.datasets.df))
# modify elements
all.datasets.df = PasteAttr(df = all.datasets.df, attr = c("tags", "diseases", "organisms", "body_parts",
                                                           "projects", "life_stages", "domains", "sources", "assays"))
# split all datasets
single.dataset.df = all.datasets.df[is.na(all.datasets.df$isCollection), ]
# dataset collection
dataset.collection.df = all.datasets.df[!is.na(all.datasets.df$isCollection), ]


base.url = "https://cells.ucsc.edu/"
collection.urls = paste0(base.url, dataset.collection.df$name, "/dataset.json")
# first-level
collection.urls.list = lapply(collection.urls, function(x){
  x.json = jsonlite::fromJSON(txt = x)
  x.df = jsonlite::flatten(x.json$datasets)
  colnames(x.df) = gsub(pattern = ".*\\.", replacement = "", x = colnames(x.df))
  x.df
})
collection.urls.df = data.table::rbindlist(collection.urls.list, fill = TRUE)
single.collection.urls.df = collection.urls.df[is.na(collection.urls.df$isCollection), ]
# second-level
collection2.urls.df = collection.urls.df[!is.na(collection.urls.df$isCollection), ]
collection2.urls = paste0(base.url, collection2.urls.df$name, "/dataset.json")
collection2.urls.list = lapply(collection2.urls, function(x){
  x.json = jsonlite::fromJSON(txt = x)
  x.df = jsonlite::flatten(x.json$datasets)
  colnames(x.df) = gsub(pattern = ".*\\.", replacement = "", x = colnames(x.df))
  x.df
})
collection3.urls.df = data.table::rbindlist(collection2.urls.list, fill = TRUE)
single.collection2.urls.df = collection3.urls.df[is.na(collection3.urls.df$isCollection), ]
# third-level
collection4.urls.df = collection3.urls.df[!is.na(collection3.urls.df$isCollection), ]
collection3.urls = paste0(base.url, collection4.urls.df$name, "/dataset.json")
collection3.urls.list = lapply(collection3.urls, function(x){
  x.json = jsonlite::fromJSON(txt = x)
  x.df = jsonlite::flatten(x.json$datasets)
  colnames(x.df) = gsub(pattern = ".*\\.", replacement = "", x = colnames(x.df))
  x.df
})
collection5.urls.df = data.table::rbindlist(collection3.urls.list, fill = TRUE)
single.collection3.urls.df = collection5.urls.df[is.na(collection5.urls.df$isCollection), ]
# fourth-level
collection6.urls.df = collection5.urls.df[!is.na(collection5.urls.df$isCollection), ]
collection4.urls = paste0(base.url, collection6.urls.df$name, "/dataset.json")
collection4.urls.list = lapply(collection4.urls, function(x){
  x.json = jsonlite::fromJSON(txt = x)
  x.df = jsonlite::flatten(x.json$datasets)
  colnames(x.df) = gsub(pattern = ".*\\.", replacement = "", x = colnames(x.df))
  x.df
})
collection7.urls.df = data.table::rbindlist(collection4.urls.list, fill = TRUE)


# recursively extract samples
ExtractSample = function(df){
  base.url = "https://cells.ucsc.edu/"
  if(!"isCollection" %in% colnames(df)){
    return(df)
  }else{
    cf = df[!is.na(df$isCollection), ]
    sf = df[is.na(df$isCollection), ]
    cu = paste0(base.url, cf$name, "/dataset.json")
    cul = lapply(cu, function(x){
      x.json = jsonlite::fromJSON(txt = x)
      x.df = jsonlite::flatten(x.json$datasets)
      colnames(x.df) = gsub(pattern = ".*\\.", replacement = "", x = colnames(x.df))
      x.df
    })
    cu.df = data.table::rbindlist(cul, fill = TRUE)
    df = data.table::rbindlist(list(sf, cu.df), fill = TRUE)
    # return(list(sf, ExtractSample(cu.df)))
    return(data.table::rbindlist(list(sf, ExtractSample(cu.df)), fill = TRUE))
  }
}
all.samples.df = ExtractSample(all.datasets.df) %>% as.data.frame()
# modify columns
all.samples.df = PasteAttr(df = all.samples.df, attr = c("tags", "diseases", "organisms", "body_parts",
                                                         "projects", "life_stages", "domains", "sources", "assays"))
# remove unused columns
all.samples.df = all.samples.df %>% dplyr::select(-c("md5", "hasFiles", "isCollection", "datasetCount", "collectionCount"))
# add label
all.samples.df.single = all.samples.df %>% dplyr::filter(!grepl(x = name, pattern = "/"))
all.samples.df.multi = all.samples.df %>%
  dplyr::filter(grepl(x = name, pattern = "/")) %>%
  dplyr::mutate(parent = gsub(pattern = "([^/]*).*", replacement = "\\1", x = name))
all.datasets.df.multi = all.datasets.df[!is.na(all.datasets.df$isCollection), c("shortLabel", "name", "tags", "organisms", "body_parts", "diseases", "projects")]
colnames(all.datasets.df.multi) = gsub(pattern = "^", replacement = "parent_", x = colnames(all.datasets.df.multi))
all.samples.df.multi = merge(all.samples.df.multi, all.datasets.df.multi, by.y = "parent_name", by.x = "parent")
# modify elements
InheritParient = function(df, attr){
  for (at in attr){
    df[[at]] = ifelse(df[[at]] == "",  ifelse(df[[paste0("parent_", at)]] == "", "", paste0(df[[paste0("parent_", at)]], "|parent")), df[[at]])
  }
  return(df)
}
all.samples.df.multi = InheritParient(df = all.samples.df.multi, attr = c("tags", "organisms", "body_parts", "diseases", "projects"))
# modify label
colnames(all.samples.df.multi) = gsub(pattern = "^shortLabel$", replacement = "subLabel", x = colnames(all.samples.df.multi))
colnames(all.samples.df.multi) = gsub(pattern = "^parent_shortLabel$", replacement = "shortLabel", x = colnames(all.samples.df.multi))
# remove parent columns
all.samples.df.multi = all.samples.df.multi %>% dplyr::select(!starts_with("parent"))
# final dataframe
all.samples.final = data.table::rbindlist(list(all.samples.df.single, all.samples.df.multi), fill = TRUE) %>% as.data.frame()
all.samples.final = all.samples.final[c("shortLabel", "subLabel", "name", "tags", "body_parts", "diseases", "organisms",
                                        "projects", "life_stages", "domains", "sources", "sampleCount", "assays")]


# 需要的函数
library(tidyverse)
PasteAttr = function(df, attr){
  for (at in attr){
    df[[at]] = sapply(df[[at]], function(x){ paste0(x, collapse = ", ") })
  }
  return(df)
}
# recursively extract samples
ExtractSample = function(df){
  base.url = "https://cells.ucsc.edu/"
  if(!"isCollection" %in% colnames(df)){
    return(df)
  }else{
    cf = df[!is.na(df$isCollection), ]
    sf = df[is.na(df$isCollection), ]
    cu = paste0(base.url, cf$name, "/dataset.json")
    cul = lapply(cu, function(x){
      x.json = jsonlite::fromJSON(txt = x)
      x.df = jsonlite::flatten(x.json$datasets)
      colnames(x.df) = gsub(pattern = ".*\\.", replacement = "", x = colnames(x.df))
      x.df
    })
    cu.df = data.table::rbindlist(cul, fill = TRUE)
    # df = data.table::rbindlist(list(sf, cu.df), fill = TRUE)
    # return(list(sf, ExtractSample(cu.df)))
    return(data.table::rbindlist(list(sf, ExtractSample(cu.df)), fill = TRUE))
  }
}
# inherit features from parent cat.
InheritParient = function(df, attr){
  for (at in attr){
    df[[at]] = ifelse(df[[at]] == "",  ifelse(df[[paste0("parent_", at)]] == "", "", paste0(df[[paste0("parent_", at)]], "|parent")), df[[at]])
  }
  return(df)
}

# extract sample attribute
ExtractDesc = function(lst, attr){
  at.list = list()
  for (atn in names(attr)){
    at = attr[atn]
    if (at %in% names(lst)){
      at.value = paste0(lst[[at]], collapse = ", ")
    }else{
      at.value = ""
    }
    at.list[[atn]] = at.value
  }
  return(as.data.frame(at.list))
}


#' Show All Available Datasets in UCSC Cell Browser.
#'
#' @return Dataframe contains all available datasets.
#' @importFrom magrittr %>%
#' @importFrom jsonlite fromJSON flatten
#' @importFrom data.table rbindlist
#' @importFrom dplyr select filter mutate starts_with
#' @export
#'
#' @examples
#' # ucsc.cb.samples = ShowCBDatasets()
ShowCBDatasets = function(){
  # parse all datasets json
  all.datasets.json = jsonlite::fromJSON(txt = "https://cells.ucsc.edu/dataset.json")
  all.datasets.df = jsonlite::flatten(all.datasets.json$datasets)
  colnames(all.datasets.df) = gsub(pattern = ".*\\.", replacement = "", x = colnames(all.datasets.df))
  # modify elements
  all.datasets.df = PasteAttr(df = all.datasets.df, attr = c("tags", "diseases", "organisms", "body_parts",
                                                             "projects", "life_stages", "domains", "sources", "assays"))
  # extract all samples
  all.samples.df = ExtractSample(all.datasets.df) %>% as.data.frame()
  # modify columns
  all.samples.df = PasteAttr(df = all.samples.df, attr = c("tags", "diseases", "organisms", "body_parts",
                                                           "projects", "life_stages", "domains", "sources", "assays"))
  # remove unused columns
  all.samples.df = all.samples.df %>% dplyr::select(-c("md5", "hasFiles", "isCollection", "datasetCount", "collectionCount"))
  # add label
  all.samples.df.single = all.samples.df %>% dplyr::filter(!grepl(x = name, pattern = "/"))
  all.samples.df.multi = all.samples.df %>%
    dplyr::filter(grepl(x = name, pattern = "/")) %>%
    dplyr::mutate(parent = gsub(pattern = "([^/]*).*", replacement = "\\1", x = name))
  all.datasets.df.multi = all.datasets.df[!is.na(all.datasets.df$isCollection), c("shortLabel", "name", "tags", "organisms", "body_parts", "diseases", "projects")]
  colnames(all.datasets.df.multi) = gsub(pattern = "^", replacement = "parent_", x = colnames(all.datasets.df.multi))
  all.samples.df.multi = merge(all.samples.df.multi, all.datasets.df.multi, by.y = "parent_name", by.x = "parent")
  # modify elements
  all.samples.df.multi = InheritParient(df = all.samples.df.multi, attr = c("tags", "organisms", "body_parts", "diseases", "projects"))
  # modify label
  colnames(all.samples.df.multi) = gsub(pattern = "^shortLabel$", replacement = "subLabel", x = colnames(all.samples.df.multi))
  all.samples.df.multi$subLabel = as.character(all.samples.df.multi$subLabel)
  colnames(all.samples.df.multi) = gsub(pattern = "^parent_shortLabel$", replacement = "shortLabel", x = colnames(all.samples.df.multi))
  all.samples.df.multi$shortLabel = as.character(all.samples.df.multi$shortLabel)
  # remove parent columns
  all.samples.df.multi = all.samples.df.multi %>% dplyr::select(!dplyr::starts_with("parent"))
  # final dataframe
  all.samples.final = data.table::rbindlist(list(all.samples.df.single, all.samples.df.multi), fill = TRUE) %>% as.data.frame()
  all.samples.final = all.samples.final[c("shortLabel", "subLabel", "name", "tags", "body_parts", "diseases", "organisms",
                                          "projects", "life_stages", "domains", "sources", "sampleCount", "assays")]
  # add sample information
  base.url = "https://cells.ucsc.edu/"
  desc.files = paste0(base.url, all.samples.final$name, "/desc.json")
  names(desc.files) = all.samples.final$name
  desc.list = lapply(names(desc.files), function(x){
    sd.json = jsonlite::fromJSON(txt = desc.files[x])
    used.attr = c("title" = "title", "paper" = "paper_url", "abstract" = "abstract", "matrix" = "matrixFile", "unit" = "unitDesc",
                  "coords" = "coordFiles", "methods" = "methods", "geo" = "geo_series")
    sd.df = ExtractDesc(lst = sd.json, attr = used.attr)
    sd.df$name = x
  })
  desc.df = do.call(rbind, desc.list)
  all.samples.final = merge(all.samples.final, desc.df, by = "name")
  all.samples.final$matrix = basename(all.samples.final$matrix)
  # return value
  return(all.samples.final)
}

# 数据的desc：项目的信息，包括title，摘要等，但是不包括metadata信息
# 数据的dataset.json：包括一些详细的信息，但是其实desc就够了，但是对于一些存在子集的数据，这个可以获取子集的名称
dataset.dataset.json = jsonlite::fromJSON(txt = "https://cells.ucsc.edu/evocell/dataset.json")

dataset.desc.json = jsonlite::fromJSON(txt = "https://cells.ucsc.edu/cortex-dev/desc.json")
dataset.desc.json = jsonlite::fromJSON(txt = "https://cells.ucsc.edu/xena/zeisel2015/desc.json")

dataset.desc.df = data.frame(title = dataset.desc.json$title, paper = paste0(cortex.json$paper_url, collapse = ", "),
                             abstract = dataset.desc.json$abstract, matrix = basename(dataset.desc.json$matrixFile),
                             coords = paste0(dataset.desc.json$coordFiles, collapse = ", "), method = dataset.desc.json$methods)

# 提取cluster
# 这里最后感觉没有必要，因为下载的metadata里面最后都会包含这个
test = jsonlite::fromJSON(txt = "https://cells.ucsc.edu/adultPancreas/dataset.json")
test = jsonlite::fromJSON(txt = "https://cells.ucsc.edu/zeisel2015/dataset.json")

cid = test$clusterField
meta.fields = test$metaFields
cid.idx = which(meta.fields$name == cid)
test$metaFields$valCounts[cid.idx][[1]] %>% as.data.frame()

# 筛选样本用于后续分析
# shortLabel, body_part, diseases, organisms, projects

df = ucsc.cb.samples
column = "shortLabel"
ucsc.cb.samples$shortLabel = as.character(ucsc.cb.samples$shortLabel)

# 大小写敏感：subLabel(会有一样的，比如all)，diseases, organisms
# 大小写不敏感：projects, shortLabel, body_parts
# 含有逗号的：body_parts, diseases, organisms，projects
CheckParas = function(df, column, para.value, fuzzy.match = TRUE){
  # convert to lower case to avoid case-sensitive
  all.values = gsub("\\|parent$", replacement = "", x = tolower(df[[column]]))
  if(is.null(para.value)){
    message("Use all ", column, " as input!")
    # return row index
    value = 1:nrow(df)
  }else{
    para.value = tolower(para.value)
    # deal with fuzzy match
    if(fuzzy.match){
      value.list = sapply(para.value, function(x) grep(pattern = x, x = all.values, fixed = TRUE))
      value.list.len = sapply(value.list, function(x) length(x))
      invalid.value = names(value.list.len[value.list.len==0])
      value = unique(unlist(value.list))
    }else{
      if(column %in% c("body_parts", "diseases", "organisms", "projects")){
        # value contains dot
        value.list = list()
        for (pv in para.value){
          pv.vec = c()
          for (avi in 1:length(all.values)){
            av = all.values[avi]
            av.vec = strsplit(x = av, split = ", ")[[1]]
            if(pv %in% av.vec){
              pv.vec = c(pv.vec, avi)
            }
          }
          value.list[[pv]] = pv.vec
        }
        # get invalid value
        invalid.value = setdiff(para.value, names(value.list))
        value = unique(unlist(value.list))
      }else{
        # value doesn't contain dot
        value.list = lapply(para.value, function(x){
          which(all.values %in% x)
        })
        names(value.list) = para.value
        value.list.len = sapply(value.list, function(x) length(x))
        invalid.value = names(value.list.len[value.list.len==0])
        value = unique(unlist(value.list))
      }
    }
    # print invalid value
    if(length(invalid.value)>0){
      message(paste0(invalid.value, collapse = ", "), " are not valid for ", column)
    }
    # deal with empty value
    if(length(value)==0){
      warning("There is no valid value under ", paste(para.value, collapse = ", "), " in ", column, ". This filter is invalid!")
      value = 1:nrow(df)
    }
  }
  return(value)
}


para.value = c("brain", "blood", "ddddd", "muscle")
# 如果不使用fuzzy match，会少一行
test = CheckParas(df = ucsc.cb.samples, column = "body_parts", para.value = c("brain", "blood", "ddddd", "muscle"), fuzzy.match = F)
test.df = ucsc.cb.samples[test, ]


collection = NULL
sub.collection = NULL
organ = c("brain", "blood")
disease = NULL
organism = "Human (H. sapiens)"
project = NULL

# 先生成一个dataframe，用于判断在否需要进一步地修改
hbb.sample.df = ExtractCBDatasets(organ = c("brain", "blood"), organism = "Human (H. sapiens)")
ExtractCBDatasets = function(collection = NULL, sub.collection = NULL, organ = NULL, disease = NULL, organism = NULL,
                             project = NULL, fuzzy.match = TRUE){
  # all sample dataframe
  all.samples.df = ShowCBDatasets()
  # extract row index under different filter
  collection.idx = CheckParas(df = all.samples.df, column = "shortLabel", para.value = collection, fuzzy.match = fuzzy.match)
  sub.collection.idx = CheckParas(df = all.samples.df, column = "subLabel", para.value = sub.collection, fuzzy.match = fuzzy.match)
  organ.idx = CheckParas(df = all.samples.df, column = "body_parts", para.value = organ, fuzzy.match = fuzzy.match)
  disease.idx = CheckParas(df = all.samples.df, column = "diseases", para.value = disease, fuzzy.match = fuzzy.match)
  organism.idx = CheckParas(df = all.samples.df, column = "organisms", para.value = organism, fuzzy.match = fuzzy.match)
  project.idx = CheckParas(df = all.samples.df, column = "projects", para.value = project, fuzzy.match = fuzzy.match)
  # filter on the whole dataset
  valid.idx = Reduce(intersect, list(collection.idx, sub.collection.idx, organ.idx, disease.idx, organism.idx, project.idx))
  used.sample.df = all.samples.df[valid.idx, ]
  return(used.sample.df)
}


DownloadCBDatasets = function(sample.df = NULL, collection = NULL, sub.collection = NULL, organ = NULL, disease = NULL, organism = NULL,
                      project = NULL, fuzzy.match = TRUE, merge = TRUE){
  # prepare samples for download
  if(!is.null(sample.df)){
    # use provided dataframe to download data
    used.sample.df = sample.df
  }else{
    # all sample dataframe
    all.samples.df = ShowCBDatasets()
    # extract row index under different filter
    collection.idx = CheckParas(df = all.samples.df, column = "shortLabel", para.value = collection, fuzzy.match = fuzzy.match)
    sub.collection.idx = CheckParas(df = all.samples.df, column = "subLabel", para.value = sub.collection, fuzzy.match = fuzzy.match)
    organ.idx = CheckParas(df = all.samples.df, column = "body_parts", para.value = organ, fuzzy.match = fuzzy.match)
    disease.idx = CheckParas(df = all.samples.df, column = "diseases", para.value = disease, fuzzy.match = fuzzy.match)
    organism.idx = CheckParas(df = all.samples.df, column = "organisms", para.value = organism, fuzzy.match = fuzzy.match)
    project.idx = CheckParas(df = all.samples.df, column = "projects", para.value = project, fuzzy.match = fuzzy.match)
    # filter on the whole dataset
    valid.idx = Reduce(intersect, list(collection.idx, sub.collection.idx, organ.idx, disease.idx, organism.idx, project.idx))
    used.sample.df = all.samples.df[valid.idx, ]
  }
  # parepare exp
  base.url = "https://cells.ucsc.edu/"
  sample.names = gsub(pattern = "/", replacement = "_", x = used.sample.df$name)
  # prepare exp matrix
  exp.urls = paste0(base.url, used.sample.df$name, "/exprMatrix.tsv.gz")
  names(exp.urls) = sample.names
  # out.exp.files = file.path(out.folder, paste(sample.names, "exprMatrix.tsv.gz", sep = "_"))
  # names(out.exp.files) = sample.names
  # prepare metadata
  meta.urls = paste0(base.url, used.sample.df$name, "/meta.tsv")
  names(meta.urls) = sample.names
  # out.meta.files = file.path(out.folder, paste(sample.names, "meta.tsv", sep = "_"))
  # names(out.meta.files) = sample.names
  # prepare coord
  coord.files = sapply(1:nrow(used.sample.df), function(x){
    paste0(base.url, used.sample.df$name[x], "/", strsplit(x = used.sample.df$coords[x], split = ", ")[[1]])
  })
  names(coord.files) = sample.names
  # create seurat object
  seu.obj.list = lapply(sample.names, function(x){
    message(x)
    Load2Seurat(exp.file = exp.urls[x], meta.file = meta.urls[x], coord.file = coord.files[[x]], name = x)
  })
  # merge or not
  if(isTRUE(merge)){
    seu.obj <- mergeExperiments(seu.obj.list)
    return(seu.obj)
  }else{
    return(seu.obj.list)
  }
}


# single test: read url directly
mat <- data.table::fread(exp.urls["cortex-dev"])
meta <- data.frame(data.table::fread(meta.urls["cortex-dev"]), row.names=1)
genes = mat[,1][[1]]
genes = gsub(".+[|]", "", genes)
mat = data.frame(mat[,-1], row.names=genes)
so <- Seurat::CreateSeuratObject(counts = mat, project = "adultPancreas", meta.data=meta)
# 添加coord
test = data.frame(data.table::fread('./tMinusSNE_on_WGCNA.coords.tsv.gz'), row.names=1)
meta.info = merge(meta, test, by = 0) %>% tibble::column_to_rownames(var = "Row.names")
seu.obj <- Seurat::CreateSeuratObject(counts = mat, project = "name", meta.data=meta.info)
Seurat::FeatureScatter(object = seu.obj, feature1 = 'V2', feature2 = 'V3', group.by = "WGCNAcluster", plot.cor = F)
# 自己可视化
library(ggplot2)
ggplot(data = all.coord.df, mapping = aes(x = TriMap_1, y = TriMap_2)) +
  geom_point()

exp.file = exp.urls["cortex-dev"]
meta.file = meta.urls["cortex-dev"]
coord.file = coord.files[["cortex-dev"]]
name = "cortex-dev"

# create seurat object (add coord to metadata)
Load2Seurat = function(exp.file, meta.file, coord.file = NULL, name = NULL){
  # source: https://cellbrowser.readthedocs.io/en/master/load.html
  # read matrix
  mat = data.table::fread(exp.file, check.names = FALSE)
  # read metadata
  meta <- data.frame(data.table::fread(meta.file, check.names = FALSE), row.names=1)
  # get genes
  genes = gsub(".+[|]", "", mat[,1][[1]])
  # modify mat genes
  mat = data.frame(mat[,-1], row.names=genes, check.names = FALSE)
  if(is.null(coord.file)){
    seu.obj <- Seurat::CreateSeuratObject(counts = mat, project = name, meta.data=meta)
  }else{
    # prepare coord file
    coord.list = lapply(1:length(coord.file), function(x){
      coord.name = gsub(pattern = ".coords.tsv.gz", replacement = "", x = basename(coord.file[x]))
      coord.df = data.frame(data.table::fread(coord.file[x], check.names = FALSE), row.names=1)
      colnames(coord.df) = paste(coord.name, 1:ncol(coord.df), sep = "_")
      coord.df$Barcode = rownames(coord.df)
      return(coord.df)
    })
    if(length(coord.file)==1){
      all.coord.df = coord.list[[1]]
    }else{
      all.coord.df = coord.list %>% purrr::reduce(dplyr::full_join, by = "Barcode")
    }
    # merge metadata
    meta = merge(meta, all.coord.df, by.x = 0, by.y = "Barcode", all.x = TRUE) %>%
      tibble::column_to_rownames(var = "Row.names")
    seu.obj <- Seurat::CreateSeuratObject(counts = mat, project = name, meta.data = meta)
  }
  return(seu.obj)
}

head(meta)

# 测试函数 --------
library(scfetch)
# * panglaodb ------
hs10.meta = ShowPanglaoDBMeta(specie = "Homo sapiens", protocol = c("Smart-seq2", "10x chromium"))
hsa.meta = ShowPanglaoDBMeta(specie = "Homo sapiens", protocol = c("Smart-seq2", "10x chromium"), show.cell.type = TRUE)
hsa.seu = ParsePanglaoDB(hsa.meta, merge = TRUE)

# * zenodo --------
zebrafish.df = PrepareZenodo(doi = "10.5281/zenodo.7243603")
zebrafish.seu = ParseZenodo(doi = c("1111", "10.5281/zenodo.7243603", "10.5281/zenodo.7244441"),
                            file.ext = c("rdata", "rds"), out.folder = "/Users/soyabean/Desktop/tmp/scdown/download_zenodo")

# * ucsc cell browser --------
ucsc.cb.samples = ShowCBDatasets()
hbb.sample.df = ExtractCBDatasets(organ = c("brain", "blood"), organism = "Human (H. sapiens)")
hbb.sample.seu = ParseCBDatasets(organ = c("brain", "blood"), organism = "Human (H. sapiens)")


